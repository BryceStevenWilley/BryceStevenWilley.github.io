<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="https://brycestevenwilley.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://brycestevenwilley.github.io/" rel="alternate" type="text/html" /><updated>2022-05-31T23:58:04-04:00</updated><id>https://brycestevenwilley.github.io/feed.xml</id><title type="html">Bryce Willey</title><subtitle>Bryce Willey- Access to Justice, Software for Humans</subtitle><entry><title type="html">Mystery Internet Bug: Text-to-Speech and The Notorious B.I.G.</title><link href="https://brycestevenwilley.github.io/2022/04/fpt_tts_biggie" rel="alternate" type="text/html" title="Mystery Internet Bug: Text-to-Speech and The Notorious B.I.G." /><published>2022-04-20T22:00:00-04:00</published><updated>2022-04-20T22:00:00-04:00</updated><id>https://brycestevenwilley.github.io/2022/04/fpt_tts_biggie</id><content type="html" xml:base="https://brycestevenwilley.github.io/2022/04/fpt_tts_biggie"><![CDATA[<aside>
This post was originally a <a href="https://twitter.com/wowitisbryce/status/1515794042444996610">tweet thread</a>.
I've added more details and background explanations where possible, and rewritten it for the blog format.
</aside>

<p>I love solving random bugs on the internet!
Recently, I’ve gotten more comfortable with browser development tools.
Sometimes I’ll be on a website and realize, “Hey, I can open this up and see how it works!”
As a result, I’ve had a lot of fun finding bugs on random websites and figuring out why they happen.</p>

<p>This post’s target bug is getting the landing page demo of <a href="https://fpt.ai/tts">FPT’s text-to-speech service</a> to play a mashup of Notorious B.I.G’s “Juicy” and the xx’s “VCR”. I’ll walk through all of the steps I took, explaining each the best I can.
You shouldn’t need to know Javascript or web technologies to follow along. As long as you know the general ideas of what a web page is (and that Javascript is just the code that handles interactions on a web page) you’ll be good.</p>

<h2 id="how-i-found-the-site-in-the-first-place">How I found the site in the first place</h2>

<p>I’ve been teaching myself Vietnamese for a while now, using some combination of Duolingo, the Foreign Service Institute’s <a href="https://www.fsi-language-courses.org/fsi-vietnamese-basic-course/">Vietnamese Course</a>, and <a href="https://apps.ankiweb.net/">Anki</a> (a digital flash card app).
To make flash cards for vocabulary, I need the Vietnamese word, its English translation, and an audio clip of the Vietnamese pronunciation. Getting the audio clip is the harder part; you have to find a good audio source for the sound, open up an audio editor, and clip out individual words. I started using text-to-speech (TTS) apps to avoid that complicated process. <a href="https://soundoftext.com/">Sound of Text</a> (and it’s paid version, <a href="https://hearling.com/">Hearling</a>) are useful, but they both use Vietnamese’s northern, or Hanoi, dialect. From what I can tell, in Boston most people speak the southern dialect. Looking for an alternative TTS service led me to <a href="https://fpt.ai/tts">FPT.ai</a>, a Vietnamese based software company.</p>

<h2 id="reproducing-the-bug">Reproducing the bug</h2>

<p>The front page of FPT’s text-to-speech service has a place where you can try it out yourself, and it works fine. If you copy my favorite tongue twister, “Tôi la, ‘Lá lạ là lá độc!’” (I yell, “the strange leaf is the poison leaf!”) into the text area, and click “Generate Audio”, there’s a short loading sound, followed by the text-to-speech audio.</p>

<p>However, we came here to get generate the <em>southern accent</em>. If you select the “Lan Nhi” accent, and type “dạ” (“yes”, one of the words that is pronounced differently between the north and south), the loading sound keeps playing. Then… Biggie starts singing?</p>

<p><img src="/assets/biggie-tts/form.png" alt="A web form on the fpt.ai/tts site. The first box is where you enter vietnamese text, the second lets you choose you voice and accent, and the third lets you say how fast to read the text. At the bottom of the image, three dots, the loading icon, are moving, indicating the sound is still loading." /></p>

<p>This keeps on for the entire song. The lines are from <a href="https://www.youtube.com/watch?v=_JZom_gVfuw">Juicy</a>, but the instrumental is different. Another thing to note is that the loading icon keeps repeating, so we can guess it’s that the text-to-speech service isn’t returning this response.</p>

<p>What specifically hooked me about this weirdness is that FPT’s site seems otherwise well developed. It looks pretty sleek, and the rest of the site works for from what I can tell. So finding what seemed like an easter egg on the product’s front page was surprising.</p>

<h2 id="mystery-1-why-biggie">Mystery 1: Why Biggie?</h2>

<p>First thing I always do when something unexpected happens on a website is to go to the dev tools! The keyboard shortcut to get there in either Firefox or Chrome is F12 or Control-Shift-J. There’s a whole lot going on in browser dev tools, but we’re only going to need 3 tabs:</p>

<ul>
  <li>Console: shows any logs from the webpage, let’s you type Javascript to run</li>
  <li>Debugger (called “Sources” in Chrome): shows you all of the page’s Javascript code</li>
  <li>Network: shows all of the network requests from your browser to the web site.</li>
</ul>

<p>First, click on the “Console tab”. We’re lucky enough that the most recent console log completely gives away the weirdest part of this mystery.</p>

<p><img src="/assets/biggie-tts/https-warning.png" alt="A console log line that says: &quot;Loading mixed (insecure) display content “http://kolber.github.io/audiojs/demos/mp3/juicy.mp3” on a secure page&quot;" /></p>

<p>This warning tells us that the audio playing is being served over HTTP, not HTTPS. Decomposing the jargon, HTTP (which stands for HyperText Transfer Protocol) is what the web runs on. Any time you visit a page on a website, your browser uses HTTP to ask the website for that specific page, and the website uses HTTP to send the page back to you.
HTTPS uses asymmetric key encryption to keep those pages secret from any one in the middle who might be listening.</p>

<aside>
Fun fact: before HTTPS was common, you could easily copy other people's cookies on the same wifi network as you and literally sign in as them on Facebook. There was even a <a href="https://codebutler.github.io/firesheep/">Firefox extension that did it for you</a>.
</aside>

<p>This warning happens because the Javascript code on the page is using plain HTTP to download the song, even though the page’s original connection is made using HTTPS.</p>

<p>Back to the main mystery, the console gives us the exact URL of the song: <a href="http://kolber.github.io/audiojs/demos/mp3/juicy.mp3">http://kolber.github.io/audiojs/demos/mp3/juicy.mp3</a>.</p>

<audio controls="controls" src="http://kolber.github.io/audiojs/demos/mp3/juicy.mp3"> </audio>

<p>The root page of that site is just <a href="http://kolber.github.io/audiojs/">the landing page</a> for <a href="https://github.com/kolber/audiojs">audio.js</a>, a Javascript library that plays audio, either using HTML 5 features or Flash apps, back when they were a thing. If we look around the GitHub for the project, we can find that they replaced <code class="language-plaintext highlighter-rouge">juicy.mp3</code> with royalty-free Creative Commons licensed tracks <a href="https://github.com/kolber/audiojs/pull/205">back in 2016</a>. And going even further back, we can find that they removed the <code class="language-plaintext highlighter-rouge">juicy.mp3</code> file from the main project <a href="https://github.com/kolber/audiojs/commit/eb3f2bb63e7a9c986f05f30270930ba0a94ff3b0">back in 2010</a>. However, the mp3 files stayed on the <a href="https://github.com/kolber/audiojs/tree/gh-pages">GitHub pages website</a>, and weren’t updated when the main project replaced everything with the Creative Commons tracks.</p>

<p>If we download the <a href="https://github.com/kolber/audiojs/blob/gh-pages/demos/mp3/juicy.mp3">MP3 itself</a>, we get the MP3’s album cover as well:</p>

<p><img src="/assets/biggie-tts/notorious-xx-album-cover.jpg" alt="The Album cover of the mystery song. The top left says &quot;...?&quot;, the top right says, &quot;wait what&quot;, and the bottom says &quot;the notorious xx&quot;, with a &quot;Parental Advisory: Explicit Content&quot; sticker in the bottom left. The middle is the mask in the shape of an X over the face of Biggie Smalls" /></p>

<p>The <a href="https://waitwhat.bandcamp.com/album/the-notorious-xx">whole album</a> is a mashup of Biggie and the xx by <a href="http://waitwhatmusic.com/">waitwhat</a>, who does all sorts of indie-rock hip-hop mashups. The instrumental is the <a href="https://www.youtube.com/watch?v=gI2eO_mNM88">xx’s VCR</a>. The mash-up is pretty good in my opinion,
but it’s probably not the choice I would have made for the demo code of a library.</p>

<p>But we still haven’t answered an important question, where is this mashup even being played? It’s included as a demo in <code class="language-plaintext highlighter-rouge">audio.js</code>, but none of the library
code itself is playing it. To answer this, we’d need to be able to search through all of the HTML, Javascript, and CSS code that is downloaded when we visit a webpage. Good thing we can do just that!</p>

<ul>
  <li>We can go to the “Debugger” tab (“Sources” in Chrome) of our still-open dev-tools.</li>
  <li>In Firefox, click anywhere in the tab and press Control-Shift-F” to bring up the “find in files” search bar
    <ul>
      <li>in Chrome, you can go to the “Sources” tab, and right click “Top”, then click “Search all files”</li>
    </ul>
  </li>
  <li>From there, we can search for “juicy.mp3”.</li>
</ul>

<p>We can see that the <code class="language-plaintext highlighter-rouge">functions.js</code> loads the mashup every time someone clicks on button (specifically, any HTML element with the <code class="language-plaintext highlighter-rouge">form-ai-speed</code>, <code class="language-plaintext highlighter-rouge">form-ai-btn</code>, or <code class="language-plaintext highlighter-rouge">btn</code> classes). Mystery solved!</p>

<p><img src="/assets/biggie-tts/debugger.png" alt="A view of the Firefox dev tools. We are searching all of the files from fpt.ai for the word &quot;juicy&quot;, which shows up in the functions.js file, being called in the audioJs() function. An `.on('click')` event handler is being added to the .form.ai-btn class." /></p>

<h2 id="mystery-2-why-dạ">Mystery 2: Why “dạ”</h2>

<p>We aren’t quite done yet: why did the input “dạ” trigger the bug, but not anything else? To figure this out, click over to the “Network” tab of the dev tools.
This tab lets you look at all of the information going between your browser and the website your visiting (or other sites).
When you get to the tab, refresh the page, wait for everything to load, then click the Trash can icon (in Chrome it’s the 🚫 icon) to clear up all of the irrelevant requests. Then, copy “dạ” in, and click “Generate Audio”. You’ll get a few new requests pop up.</p>

<p><img src="/assets/biggie-tts/network-firefox.png" alt="The network tab of Firefox. There are three network requests, one to `kolber.github.io`, one to `www.google.com`, and one to `demo.fpt.ai`" />.</p>

<p>I have an ad blocker on my browser; without it you’ll probably see a few more requests going to tracking sites and Facebook. These are the important ones:</p>

<ul>
  <li>a GET for the <code class="language-plaintext highlighter-rouge">juicy.mp3</code> that starts playing. From our previous sleuthing that, we know this happens immediately when the button is clicked.</li>
  <li>a POST to <code class="language-plaintext highlighter-rouge">www.google.com</code>. We can see that the initiator is <code class="language-plaintext highlighter-rouge">recaptcha__en.js</code>. <a href="https://developers.google.com/recaptcha/">reCAPTCHA</a> is the “I’m not a robot” checkbox you see on a lot of pages, and in this case is here to make sure that we’re not a bot taking advantage of FPT’s API without making an account first.</li>
  <li>a POST to <code class="language-plaintext highlighter-rouge">demo.fpt.ai</code>. This is the one we want to look at! If you click on that line, you’ll see some more info about the request, and if you go to the “Response” tab of the request, you’ll see that there was an error: “Body must have at least 3 characters”. Which explains it: even though “dạ” is a valid word, the API isn’t meant to handle individual words, it’s built for full sentences and longer paragraphs.</li>
</ul>

<h2 id="conclusions">Conclusions</h2>

<p>There’s not a clear moral of this story. As cool as it is, maybe don’t use unlicensed mashups in your demo projects; it’s caused some legal issues with other projects before and <a href="https://github.blog/2020-11-16-standing-up-for-developers-youtube-dl-is-back/">caused them to be copyright-striked</a> until they <a href="https://github.com/animelover1984/youtube-dl/commit/0851123c1909558268e8e237214d9c466cf5198d">removed the offending content</a>.</p>

<p>My take-away is that doing detective work like this is fun. I’ve started doing it a lot more since getting comfortable with Javascript and web dev in general. <a href="https://jvns.ca/blog/2022/03/10/how-to-use-undocumented-web-apis/">Julia Evans wrote another great post</a> about doing this sort of thing with web APIs.</p>

<p>If you want to dive more into dev tools, you can go to <a href="https://firefox-dev.tools/">Firefox’s documentation</a>, or <a href="https://developer.chrome.com/docs/devtools/">Chrome’s documentation</a>.</p>]]></content><author><name></name></author><category term="software" /><category term="bug-hunt" /><category term="web" /><category term="javascript" /><summary type="html"><![CDATA[Solving random bugs on the internet with Browser Dev Tools]]></summary></entry><entry><title type="html">Readability Scores Pt 2: Observations</title><link href="https://brycestevenwilley.github.io/2022/02/readability-scores-obs" rel="alternate" type="text/html" title="Readability Scores Pt 2: Observations" /><published>2022-02-21T10:00:00-05:00</published><updated>2022-02-21T10:00:00-05:00</updated><id>https://brycestevenwilley.github.io/2022/02/readability-scores-obs</id><content type="html" xml:base="https://brycestevenwilley.github.io/2022/02/readability-scores-obs"><![CDATA[<p>This is the second post in a series. If you want more background on what readability scores are or detailed write ups on individual scores, check out the <a href="/2022/01/readability-scores-reference">first piece</a> in this series.</p>

<p>So now that we know about many readability scores, what should we do with them? Which ones should we use, and which ones should we avoid?
This post is a bit more wishy-washy than the last one; most of the info here is just my observations and insights into readability.</p>

<h2 id="why-are-there-so-many">Why are there so many?</h2>

<p>One question that I wanted to answer when I started looking into readability scores: why are there so many different scores?
A quote from Edward Fry <a href="/2022/01/readability-scores-reference#fry-readability-graph">when talking about the Fry Reading Graph score</a> helps us answer this question:</p>

<blockquote>
  <p>Though readability formulas are used by some
 teachers, some librarians, and some publishers, their number is all
 too few. Perhaps the sheer time it takes to apply these formulas
 causes them mostly to languish in term papers and occasional magazine articles.</p>
</blockquote>

<p>This is a key point to me; in the 60’s, most readability scores were trying
to do the exact same thing, just with slightly more efficient algorithms. Which makes sense in a way.</p>

<p>For example, lets see how long it takes someone to actually calculate a reading grade by hand.
I started at a random point in a paper I was reading, and timed myself to see how long it took me to count 100 words, and the number of syllables and sentences in those 100 words <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.
It took me a 4 minutes and 22 seconds to count out 100 words and then to count the 5 sentences and 177 syllables in that 100 words.
By far the longest part was counting the words and sounding out and counting the syllables.
I then timed myself on the on-paper math calculations for Fry’s readability graph and Flesch-Kincaid.</p>

<ul>
  <li>Fry’s graph took 15 seconds to locate the point (off the chart, lol).</li>
  <li>Flesch-Kincaid took me 2 minutes 42 seconds to calculate a grade level of 13.</li>
</ul>

<p>This very much proves Fry’s point: the by-hand math calculations still took about ~40% of the time. I am a bit out of practice at calculation by-hand, but I’d guess that with some practice, about 20% of the time to calculate the Flesch-Kincaid grade would still be the math.
Coming up with a new score that didn’t need exact calculations and saved 20% of the time to take to make the score would be a big improvement.</p>

<p>But now, we have computers. All of the counting and calculations that I just did happens essentially instantaneously.
Slightly faster scores like Fry’s or SMOG don’t have advantage of being that much faster anymore.</p>

<p>There is a different reason why new readability scores are still being developed today though; different groups of people think and read differently.
Diversity has always been a good thing, and some of the research done on how <a href="/2022/01/readability-scores-reference#multi-feature-scores-for-efl-learners">English-as-an-additional-Language learners</a> or <a href="/2022/01/readability-scores-reference#intellectual-disabilities">people with intellectual disabilities</a> read shows that we all think and process the written word differently.
Obviously a single formula that takes into account 2 shallow pieces of info about the text aren’t going to be great predictors when it comes to these different populations.</p>

<h2 id="whats-missing-from-the-standard-readability-scores">What’s missing from the standard readability scores?</h2>

<p>By standard readability scores, I mean the <a href="/2022/01/readability-scores-reference#simple-counts">simple counts</a> and the <a href="/2022/01/readability-scores-reference#word-lists">word list</a> scores. 90% of the time, when someone is talking about readability
scores, they’re talking about one of those.</p>

<p>As the previous paragraph shows, most readability scores were designed to be easier to compute; the fewer things to count and calculate, the better.
Arguably, they are much too simple. While McLaughlin is right that “It is not essential that the variables in a prediction formula should have direct causal connection with the quantity that is being predicted,” if you’re trying to use a readability score to improve writing, it is almost always true that your method of improvement should have a causal connection.</p>

<p>Another things that is lacking from standard readability scores is that they are validated on novel or article-like text. Those types of text miss a huge type of writing that many of do nowadays: website and social media writing. For example, readability of websites was the main goal of <a href="#Collins-Thompson-Callan-Score">Collins-Thompson and Callan</a> when they introduced their unigram model. We can get a bit more qualitative data on real users from <a href="https://aclanthology.org/U19-1003/">Jacob and Uitdenborgerd (2019)</a>. They looked at the readability of tweets on Twitter. On twitter, it’s difficult to grade short content as readable or not, partially because of the social media and
conversationality (“Slang, wrongly written and uncommon words seem to lower the readability”), and partially because of Twitter’s exceptionally short length.
They also hint that there seems to be only so much you can do to increase readability in short form text, given that readability is more dependent of demographics. Also that familiarity to the medium is important: all your efforts to make something readable will be in vain if that person has never used a modern website before.</p>

<h2 id="more-specific-validations">More specific validations</h2>

<p>The grade levels of readability scores are kinda made up.
Yes, some of the scores are validated based on the grade level of their readers; if more than 50% of a sample of 4th graders
could read a text, that text is considered to be 4th grade level.
However, there are a lot of issues with the actual labeling of a text as “4th grade”.</p>

<ul>
  <li>they were validated on 4th graders in 1975 (or whenever the score was last updated), not 4th graders today.</li>
  <li>it implies that you need to write like you’re talking to a 9 year old, which implies that you need to dumb it down. You don’t need to dumb down your writing!
A great point that my colleague <a href="https://www.mlri.org/staff-member/caroline-robinson/">Caroline Robinson</a> has made is that <strong>a reading grade level isn’t a thinking grade level</strong>: an adult who reads at a lower than average grade level can talk about complicated topics and ideas.
Additionally, people are often able to read above their average readability ! The <a href="/2022/01/readability-scores-reference#flesch-reading-ease">original Flesch score</a> tried to consider interest and appeal in the article topic into account in the score.</li>
  <li>it implies that everyone who does read above a 4th grade level will be able to read the text.
People also often read below their readability score. Not everything that a college reading grade level person reads will be at a 13th grade level.
They’ll generally want to read easier text most of the time, since reading difficult texts is hard for everyone.
Even if you read at a higher grade level, easier readability <a href="https://en.wikipedia.org/wiki/Readability#Readability_and_newspaper_readership">improves comprehension and reading speed</a>.</li>
</ul>

<p>I think that readability scores could be split into two things:</p>

<ul>
  <li>a score that is just a percentage of the population (maybe targeted to your specific population!) that could read your text. It’s judgement free, and you aren’t dumbing your content down. It could be actually validated as well: instead of only looking at 4th graders and seeing what percentage of them pass the comprehension tests, just sample
the general population, and give a range of the percentage of the population that could read it. Many of the techniques that are used in electoral polling might be useful here.</li>
  <li>a score that shows how difficult it is to read your text. It might be accessible to 75% of the general population, but if it’s exhausting for 90% of the population to read, then that’s going to determine how many people read and comprehend your article more than the other score.</li>
</ul>

<h2 id="what-readability-score-should-i-use">What readability score should I use?</h2>

<p>If you want my (pretty amateur opinion), I’d lean towards some mix / weighted average of Flesh, Dale-Chall, and SMOG. Mostly given that all of them have their draw backs, and I like word-list methods a lot.</p>

<p>However, given all of the above, I have a potential hot take: when writing yourself, or if you are developing tools to help people improve their writing, (like I am right now), you shouldn’t show the author the readability scores of their text directly. If you do show a readability score, show it to some final gatekeeper who isn’t the author.</p>

<h2 id="readability-tools-and-plain-language">Readability Tools and Plain Language</h2>

<p>This might over-complicate things, but hear me out: you can think of writing a readable text as an optimization problem.
In optimization, you have a problem space, some constraints, a starting point, and some optimal solution, and some objective function to measure that optimal solution.
For example, you have a car factory, and you want to optimize the process of assembling a car. The problem space is all of the different
ways you can put together a car. The constraints are that you want to end up with a working, high quality car. The starting point is the current way
you are putting together the car, and the optimal solution is a faster way to put together the car. The objective function is time: faster is better.
You also have these 5 things with our readability problem: the problem space is the space of all english texts, the constraints are that you want to communicate a particular set of ideas and concepts, you have a first draft, and you have your goal text, that explains the same ideas but with a lower difficultly grade. Our objective function here is readability.</p>

<p>In optimization, there’s generally two ways of improving your current point:</p>

<ul>
  <li>you can use information about your current point to try to improve it slightly, known as hill-climbing (you want to go up the hill from where you are).</li>
  <li>you can jump to a random point to see if it’s better than your last guess.</li>
</ul>

<p>In combination these two ideas are very powerful. In readability, these are the same as making slight phrase or sentence edits to your text, and just rewriting the whole thing.
I’d argue that most of editing is the hill-climbing part, as you aren’t going to be constantly rewriting your whole article.
However, for hill-climbing, you need some way to narrow your focus in on a problem. Despite the fact that our objective function is a readability score, readability scores are not good at hill-climbing at all.
Pretty much every author of a readability score has said that scores are approximations that happen to correlate with readability.
Scores do <strong>not</strong> measure causation. Specifically, scores correlate well with naturally written text, but they likely do not correlate well with
edited text that was guided by said readability scores.
You can’t just take an existing text, shorten the sentences by replacing “, and” with a period or automatically swap
words with shorter synonyms, and expect it to actually read easier.</p>

<blockquote>
  <p>If the read­ing level is too high, the easiest fix is to divide long sentences into shorter ones.
This kind of chopping will indeed lower the readability level, but it’s often counter­ productive.</p>

  <ul>
    <li><a href="https://www.semanticscholar.org/paper/Readability-formulas%3A-Useful-or-useless-McClure/7c5a5c88214ead4552eded309ec26995013faac9">Dr. J Peter Kincaid</a></li>
  </ul>
</blockquote>

<p>So, for the majority of your writing effort, you likely don’t want a readability score; you want a readability tool.</p>

<p>My best suggestion here is to use something like the <a href="https://hemingwayapp.com/">Hemingway editor</a>, or to learn about (and use) <a href="https://www.plainlanguage.gov/">plain language</a>.
Plain language is essentially a list of guidelines that encourage you to write simpler. It’s not about dumbing down your writing, just about being clearer.
If you work for the US federal government, you are <a href="https://www.plainlanguage.gov/law/">literally required</a> use plain language to improve readability.</p>

<p>It’s possible that more modern readability scores (<a href="/2022/01/readability-scores-reference#multi-feature-methods">multi-feature methods</a>) are better suited as objective functions and for hill-climbing.
But I’d need to see some research on it before I believe it.</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>Here’s the paragraph: “Measuring the usability of these IVIS as early as possible during the development cycle plays a key role when designing for the automotive context (Harvey &amp; Stanton, 2013). On the one hand, poor customer experience can be the result if usability is neglected in the development. Beyond the tasks performed using the IVIS, the driver of a car must always concentrate on the actual driving task. From a usability perspective this dual-task scenario represents a special challenge during the evaluation of an HMI. However, most car manufacturers are facing strict confidentiality guidelines in early development stages.” <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="civic" /><category term="a2j" /><category term="equity" /><category term="nlp" /><summary type="html"><![CDATA[Asking about what readability scores are actually good for]]></summary></entry><entry><title type="html">Readability Scores Pt 1: Reference Guide</title><link href="https://brycestevenwilley.github.io/2022/01/readability-scores-reference" rel="alternate" type="text/html" title="Readability Scores Pt 1: Reference Guide" /><published>2022-01-16T08:12:49-05:00</published><updated>2022-01-16T08:12:49-05:00</updated><id>https://brycestevenwilley.github.io/2022/01/readability-scores-reference</id><content type="html" xml:base="https://brycestevenwilley.github.io/2022/01/readability-scores-reference"><![CDATA[<p>I started looking into readability scores in mid-October 2021, and the first thing that struck me was that there are a lot of readability scores.
Most use similar methods to do the same thing, and most were verified in the same way.
So, I’m compiling more a general overview and history of the more significant and interesting scores here <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p>

<h2 id="what-is-a-readability-score">What is a readability score?</h2>

<p>A <strong>readability score</strong> rates how difficult or easy a text is to read. They’re sometimes called formulas, indices, or measures; I’ll try to stick to “score” in this post, unless I’m giving the official name or quoting someone. A <strong>text</strong> can be pretty much anything: a chapter from a novel, an instruction manual, a website, a series of tweets, etc.
The score itself is an <strong>equation</strong> that takes as input the text, calculates some <strong>features</strong> of the text (aspects of the text that can be turned into a number) and outputs a continuous number,
sometimes rounded to the closest whole number.</p>

<p>Once you have a readability score, you have to verify that it is actually “correct”, that it predicts what it says it does.
Most readability scores predict the grade level of a particular text, i.e. the earliest grade level at which you might expect a child to be able to read that text (which I do have some problems with, but that’s all in <a href="/2022/02/readability-scores-obs">the next post</a>).
So, to verify the score most researchers do one of three things:</p>

<ul>
  <li>grade texts using actual readers; if &gt; 50% of a sample of 4th graders can read and correctly answer comprehension questions about the text, then that text is considered to be at a 4th grade reading level. Researchers then adjust the readability scores until it scores those texts to also be at a 4th grade level.</li>
  <li>make experts (teachers, learning professionals, book publishers) guess the grade level of texts, and adjust the readability score until it matches the guesses</li>
  <li>make their new score correspond to existing reading scores. Researches calculate the “relative rankings” of scores, i.e. scoring and ordering texts according to each readability score. They then try to end up with the same ordering of texts between the different scores.</li>
</ul>

<p>Since most of these scores are <em>equations</em>, here are definitions of some commonly used variables in the scores:</p>

<ul>
  <li>$ words $ is the total number of words in the whole text</li>
  <li>$ sents $ is the total number of sentences in the whole text</li>
  <li>$ sylls $ is the total number of syllables in the whole text</li>
</ul>

<h2 id="simple-counts">Simple Counts</h2>

<h3 id="flesch-reading-ease">Flesch Reading Ease</h3>

<p>By far the most well known readability score (but not the most correct), the Flesch reading ease score is based on the average numbers of words per sentence and syllables per word.
It was developed by Dr. Rudolf Flesch in the <a href="https://web.archive.org/web/20160712094308/http://www.mang.canterbury.ac.nz/writing_guide/writing/flesch.shtml">1940’s</a>.</p>

<p>The Flesch reading ease’s equation is:</p>

\[206.835 - 1.015 ( \frac{words}{sents} ) \\ - 84.6 ( \frac{sylls}{words} )\]

<p>Most readability scores around in the 40’s were intended for use with children.
Teachers would use scores to decide if a text was the appropriate grade for a child: challenging but not disheartening. Flesch however developed his score to be used for adults. He tested it using the <a href="https://www.google.com/books/edition/Standard_Test_Lessons_in_Reading/DdCgAAAAMAAJ?hl=en&amp;gbpv=1&amp;printsec=frontcover">McCall-Crabbs Standard Test Lessons in Reading</a>, which were passages graded by how well sample groups of children answered comprehension questions at the end of each passage.</p>

<p>Interestingly, at one point Flesh used the number of personal pronouns (I, you, etc.) and the number of affixes to words (“anti-“ in “antitoxin” and “-able” and “-ity” in “readability”) to compute the score.
However, the people computing the score on new texts disregarded those features and only computed sentence length.
As a result, Flesch split the score into two parts, the one known today, and another one known as the “human interest” score, which focused on the number of personal pronouns.</p>

<p><a href="https://lib.dr.iastate.edu/cgi/viewcontent.cgi?article=1077&amp;context=rtd">Nancy Ann Vieth’s MA thesis</a> gives a great overview of Flesch’s development of the reading ease score, and the academic context at the time.</p>

<h3 id="flesch-kincaid-grade-level-evaluation">Flesch-Kincaid Grade Level Evaluation</h3>

<p>Like the reading ease score, the Flesch-Kincaid grade level evaluation is based on the average numbers of words per sentence and syllables per word.
It was co-developed by Flesch and J. Peter Kincaid in the 70’s for the US Navy.</p>

<p>The Flesch-Kincaid Grade Level Evaluation’s equation is:</p>

\[-15.59 + 0.39 ( \frac{words}{sents}) \\ + 11.8 (\frac{sylls}{words})\]

<p>The output is specifically the expected grade level of the text, which is why this equation looks to be the inverse of the reading ease score.</p>

<p>The grade level evaluation has more of an emphasis on shorter words, penalizing each additional average syllable per word 2.75 more than the reading ease. The <a href="https://en.wikipedia.org/wiki/Flesch%E2%80%93Kincaid_readability_tests#/media/File:Flesch_Kincaid_readability_tests.svg">Wikipedia user Cmglee</a> made a good way to visualize this, shown below.</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/83/Flesch_Kincaid_readability_tests.svg/1920px-Flesch_Kincaid_readability_tests.svg.png" alt="A 2 dimensional plot that shows how the reading ease score scales on it's 2 inputs compared to the grade level evaluation" /></p>

<p>The grade level evaluation was validated using the Gates-MacGinitie Reading test.</p>

<p>Kincaid himself has some interesting thoughts on readability scores, particularly that they shouldn’t be over applied by writers, given how easy it is to game most of them.</p>

<blockquote>
  <p>If the read­ing level is too high, the easiest fix is to divide long sentences into shorter ones.
This kind of chopping will indeed lower the readability level, but it’s often counter­ productive.</p>

  <ul>
    <li><a href="https://www.semanticscholar.org/paper/Readability-formulas%3A-Useful-or-useless-McClure/7c5a5c88214ead4552eded309ec26995013faac9">Dr. J Peter Kincaid</a></li>
  </ul>
</blockquote>

<h3 id="gunning-fog-index">Gunning Fog index</h3>

<p><a href="https://web.archive.org/web/20180912145342/http://journals.sagepub.com/doi/abs/10.1177/108056998504800203">The Gunning Fog index</a> was made by Robert Gunning, an American businessman in newspaper publishing, in 1952.
His word to describe reading difficulty is “Fog”, thus the name, Gunning Fog.
The score’s equation is:</p>

\[0.4 \frac{words}{sents} + 100 \frac{complex\_words}{sents}\]

<p>The output is the grade level of the text.
Complex words are words that have more than 3 syllables, not including several common syllables such as -ing.</p>

<p>It requires a minimum of 100 words in the text when calculating the score.
It is important to note that most scores developed before 1990 were standardized and validated on longer prose, at least a paragraph or two of text. The scores that you get with shorter amounts of text are less likely to be the true readability of the text. This fact makes it difficult to apply the more commonly used readability scores to shorter amounts of text that you would find on a webpage, or in a tweet.</p>

<h3 id="fry-readability-graph">Fry Readability Graph</h3>

<p>The Fry Readability Graph, developed by Edward Fry in 1968, is pretty straight forward.</p>

<ol>
  <li>Sample several 100 word groups from the text</li>
  <li>Calculate the number of syllables and sentences in those groups</li>
  <li>Use those calculations to place a point on a “graph” (aka a plot with word length on the x-axis and average number of sentences on the y-axis).</li>
</ol>

<p><img src="/assets/blogs/readability/fry_readability.png" alt="The Fry Readability Graph itself. The x-axis of the plot measures the average number of syllables per 100 words and goes from 108 to 172. The y-axis of the plot measure the average number of sentences per 100 words, and goes from 3.6 at the bottom to 25.0. The approximate grade levels are delimited by diagonal lines, each spaced out between   the top left and the bottom right of the graph." /></p>

<p>It was validated using “a lot of” book publisher grade level recommendations.
The lack of an actual equation, deferring to the flexible but approximate graph, probably didn’t help the score’s usage.</p>

<p>However, Fry does tell us why some many readability scores were developed in the 1950’s - early 80’s:</p>

<blockquote>
  <p>Though readability formulas are used by some
 teachers, some librarians, and some publishers, their number is all
 too few. Perhaps the sheer time it takes to apply these formulas
 causes them mostly to languish in term papers and occasional magazine articles.</p>
</blockquote>

<h3 id="smog-simple-measure-of-gobbledegook">SMOG (Simple Measure of Gobbledegook)</h3>

<p>G. Harry McLaughlin, the Editor at the Mirror newspaper and Doctor of Psycholinguistics at University of London, published his SMOG score at the University of Syracuse in 1969.
Sold as a simpler version of the <a href="#fry-readability-graph">Fry Readability Graph</a>, it consists of three steps:</p>

<ol>
  <li>Count sentences (at least 30)</li>
  <li>Count polysyllables (&gt;=3 syllables)</li>
  <li>\(1.043 \sqrt{polysyll (30 / sents)} \\ + 3.1291\) gets you the grade level.</li>
</ol>

<p>An even simpler approximation of the last step is to take the square root of the nearest perfect square of the number of polysyllables. So if there are 95 polysyllables in the sample, you round to 100, and take the square root, which means the text is at a 10th grade level.</p>

<p>McLaughlin also suggests that the text be at least 30 sentences (which he estimates to be 600 words), saying that a sample of 100 words “is so small that it may be quite uncharacteristic of the text being assessed.”</p>

<h3 id="ari-automatic-readability-index">ARI (Automatic Readability Index)</h3>

<p>The Automatic Readability Index (1967) is unique in this list because it was
created in mind with real time type-writer readability monitoring.
To do that in the 60’s, they used characters per word instead of the
usual syllables per word.</p>

\[4.71 \frac{characters}{words} \\ + 0.5 \frac{words}{sents} - 21.43\]

<h2 id="word-lists">Word Lists</h2>

<p>All of the scores above tend to reduce text to two, maybe three numerical features such as syllable or word counts, and don’t consider the content of the text.
<strong>Word lists</strong> attempt to fix this problem. They still reduce the text to some number, but this time if you changed every letter in every word, it <em>would</em> make a difference to the readability score.</p>

<p>The two methods worth listing here are Thorndike’s word list, and the Dale-Chall readability formula.</p>

<p><a href="https://www.jstor.org/stable/1471167">Thorndike’s word list</a>, first published in 1921, has about 20,000 words now, and isn’t exactly a readability measure on it’s own, but
forms the basis of other readability scores based on word lists. Thorndike’s word list is mainly ordered by frequency of words in texts, and by assumption, familiarity.</p>

<p>The <a href="https://en.wikipedia.org/wiki/Dale%E2%80%93Chall_readability_formula">Dale-Chall</a> formula was developed by Edgar Dale and Jeanne Chall in 1948. It used a list of 763 words that 4th grader were familiar with, and in 1995, was updated to include 3000 familiar words.
Assuming that any word not on the list is considered to be a <em>hard word</em>, the equation is:</p>

\[0.1579 (\frac{hardwords}{words} * 100) \\ + 0.0496 (\frac{words}{sents})\]

<p>These methods were generally used <em>before</em> the above counting methods were invented, and besides for Dale-Chall, they aren’t talked about much anymore.
Some criticism of them were that:</p>

<ul>
  <li>the lists were too long to use by hand, which is a valid argument in 1940. Reading through an 8 page list for every word in the text is very tiresome (though I can’t tell how long some of those word lists got to be). Nowadays, this argument is less useful, as there’s no meaningful performance difference on modern computers between doing O(N) set existence queries vs simple counts.</li>
  <li>the lists were very influenced by what materials the words were counted from. This is a valid point, and one that is still important today.</li>
  <li>the lists only show familiarity with the words, as words can still be abstract, ambiguous, or vague, depending on the context in which they’re used.</li>
</ul>

<h2 id="multi-feature-methods">Multi-feature Methods</h2>

<p>It might stand out that all of the readability scores above were created a while back: the latest is the Flesch-Kincaid score, from 1975.
Computing power, especially with regards to text and natural language processing (NLP), has significantly increased since the 70’s, as has the prevalence of personal computers.
With the development of NLP as a field in artificial intelligence, new methods have been used to compute readability scores.
For the purposes of naming them, we’ll call these methods <strong>multi-feature methods</strong>.</p>

<p>The key difference between multi-feature methods and the methods above are that multi-feature methods generally rely on many more features of the text. While scores like Dale-Chall and SMOG rely on 2 or 3 different features of the text like the number of sentences and the complexity of each word, multi-feature methods
rely on many more features, including grammatical analysis, named entity counts, and deep-learned features.
Due to the high number of features, these methods need to be trained on specific datasets, sometimes called a corpus. This allows them to be more specific to a particular type of text, like texts from the healthcare or legal industries, though <a href="https://en.wikipedia.org/wiki/Overfitting">overfitting</a> should be avoided.</p>

<p>Training on specific corpuses can get around a lot of the downsides mentioned in the <a href="#word-lists">word lists</a> section. The lists can’t get too long; that complexity is handled by the algorithm.</p>

<h3 id="collins-thompson-callan-score">Collins-Thompson Callan Score</h3>

<p>The Collins-Thompson-Callan score, introduced in 2004 in <a href="http://www.cs.cmu.edu/~callan/Papers/hlt04-kct.pdf">this paper</a>, outlined the basic gist of a multi-feature readability score algorithm:</p>

<ul>
  <li>gather a specific corpus of texts. Since the authors of this paper wanted to make a readability score made for web pages, they found 550 documents, each marked to a certain reading level grade by the website authors.</li>
  <li>use a variant on a naive bayes classifier to predict the “class” (the reading grade level) based on the presence of certain words. Since each word in a document is considered without it’s surrounding context, i.e. one word at a time, the authors call it a unigram model.</li>
  <li>Smooth it: the training words are smoothed across grade levels, across the language model (using Simple Good-Turing smoothing), and the final resulting grade level is the average for the top 2 predicted grades.</li>
</ul>

<h2 id="specific-target-audiences">Specific Target Audiences</h2>

<p>All of the above readability scores try to target their output to the most people possible.
However, not everyone struggles with reading comprehension for the same reasons.
Grade school students have different challenges with reading than people who are learning english as a second or third language, and people with intellectual disabilities have different challenges than either of those groups.
Lumping these groups together to try to predict a single readability score isn’t going to be the most accurate for the groups individually. The next few scores, despite using different methods (counts vs multi-feature), target one of these subpopulations.</p>

<h3 id="mcalpine-eflaw-readability-score">McAlpine EFLAW Readability Score</h3>

<p>The <a href="https://web.archive.org/web/20050217042336/www.webpagecontent.com/arc_archive/139/5/">McAlpine EFLAW score</a> was made specifically for “Global English for Global Business”. The name is a combination of “EFL”, or “English as a Foreign Language”, and “flaw”, or flaws of writing for an English as a foreign language audience.
It’s based on the idea that English learners have more trouble with clusters of shorter words of 1-3 letters, called miniwords. A phrase like “In relation to the date of a new time”, would have 7 miniwords.</p>

<p>The score isn’t normalized to grade levels, but it still increases as its difficulty does.</p>

\[(words + miniwords) / sents\]

<!--   Also 
  https://strainindex.wordpress.com/2009/04/30/mcalpine-eflaw-readability-score/
-->

<h3 id="multi-feature-scores-for-efl-learners">Multi-Feature Scores for EFL Learners</h3>

<p>The <a href="https://aclanthology.org/2020.nlptea-1.3/">LXPER Index 2.0</a> is a “text readability assessment model”, built to work for non-native language learners (specifically Korean), similarly
to the McAlipne EFLAW score above. However, it’s specifically targeted at texts in “foreign English language training curriculums”. The reading levels are targeted to grade levels so that texts will properly challenge students and improve their language learning. Additionally, it takes into account that student’s primary interaction with the language they are learning is through the curriculum.
Word lists like Dale-Chall and other readability scores assume that the reader will have broad and diverse exposure to the target language, but foreign language learners not immersed in the language will have a much more limited exposure.</p>

<p>This method is a <a href="#multi-feature-methods">multi-feature readability score</a>. Several dozen different text features, including many of the ones mentioned in other readability scores like average words per sentence and percentage of “complicated” words are all included, as well as more detailed features such as the average number of verb phrases and other grammatical structures in the text, the number of entities and lexical chains in the text, and the number of pre-graded vocabulary words.
All of these features are used in a logistic regression model, which is trained on pre-graded text samples.</p>

<h3 id="intellectual-disabilities">Intellectual Disabilities</h3>

<p>Some more advanced scores attempt to find more features that are directly correlated
to readability, like <a href="https://aclanthology.org/E09-1027/">Feng, Elhadad, and Huenerfauth</a>. Those authors focus on readability for people with intellectual disabilities, which gives some valuable insights into the actual process of readability that most readers subconsciously work with each time they read.</p>

<p>Notable from the above paper is that entity density is important. Entity density is the number of things mentioned in a sentence, and how many times they were mentioned; essentially, how much your working memory is used when reading.
Earlier studies with the intellectually disabled population showed that “these users are better at decoding words than at comprehending text meaning.” So reading longer words isn’t super hard, but trying to create a “cohesive representation of discourse” can be. They also found was that “texts designed for children may not always lead to accurate models of the readability of texts for other groups of low-literacy users”. That insight is important for readability scores in general: most were made specifically for children grade levels!</p>

<h2 id="misc-readability-scores">Misc Readability scores</h2>

<p>Other readability scores that I found in my research, but didn’t write about here.</p>

<ul>
  <li><a href="https://readabilityformulas.com/spache-readability-formula.php">Spache</a></li>
  <li><a href="https://www.jstor.org/stable/41387808">Botel</a></li>
  <li><a href="https://en.wikipedia.org/wiki/Coleman%E2%80%93Liau_index">Coleman-Liau</a>, also uses characters per word</li>
  <li><a href="https://en.wikipedia.org/wiki/Linsear_Write">Linsear Write</a></li>
  <li><a href="https://www.readabilityformulas.com/the-LIX-readability-formula.php">LIX (Läsbarhetsindex)</a>, one of the few tests developed outside of the US, in Sweeden</li>
  <li><a href="https://aclanthology.org/W16-0502/">Xia et al., 2016</a>, another multi-feature approach for EFL</li>
</ul>

<h2 id="so-what-now">So, what now?</h2>

<p>So now that we know about many readability scores, what should we do with them? Which ones should we use, and which ones should we avoid? I try to address some of those question in <a href="/2022/02/readability-scores-obs">part 2 of this blog</a></p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>When reading research papers, I always try to write something close to a background or previous work section of an academic paper. It forces me to try to understand what I’m writing about from different angles, and to do my best to cover the field I’m writing about (comparatively, since I do this in my free time, lol). But, a lot of that info doesn’t really fit in the final article written, so I decided to try to summarize it somewhere else. This post is that “somewhere else”. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="civic" /><category term="a2j" /><category term="equity" /><category term="nlp" /><summary type="html"><![CDATA[A big ol' list of readability scores, with more than just their formulas]]></summary></entry><entry><title type="html">What the World Needs: Pivoting away from Robotics</title><link href="https://brycestevenwilley.github.io/2021/06/new_job" rel="alternate" type="text/html" title="What the World Needs: Pivoting away from Robotics" /><published>2021-06-20T23:00:00-04:00</published><updated>2021-06-20T23:00:00-04:00</updated><id>https://brycestevenwilley.github.io/2021/06/new_job</id><content type="html" xml:base="https://brycestevenwilley.github.io/2021/06/new_job"><![CDATA[<p>There’s an Japanese concept about one’s purpose in life called Ikigai.<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup> It’s built on four criteria:</p>

<ul>
  <li>What you’re good at</li>
  <li>What you like doing</li>
  <li>What people will pay you to do</li>
  <li>What the world needs</li>
</ul>

<p>I like applying this concept to finding jobs; it’s a good mental model about the tradeoffs in different opportunities, and it has illuminated my thoughts and past choices around my own career.
Up until sophomore year of undergrad, most choices I made about school and my job revolved around what I liked doing:
programming.<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup> I’m lucky enough this choice also happened to satisfy two other criteria: I’m a good enough software engineer, and software engineering pays well.</p>

<p>The final criteria has been tricky though: what the world needs. It was most certainly been a blind spot in
my choice of career for a long time. I got some internships at Google in undergrad, and started researching and pursuing a Master’s Degree in the robotics,
not because I thought the world really needed it, but because I liked the work.
Robotics has a real geometry and permanence to it that other software focuses like web design or databases lack, and still has some incredibly interesting and hard problems.</p>

<p>However once I graduated with my Master’s, moved to Boston, and started working for a robotics startup, I had a bit more time on my hands. Slowly, I started feeling the itch of “what the world needs.”</p>

<h2 id="robotics-and-what-the-world-needs">Robotics and What the World Needs</h2>

<p>I always found robotics to be a morally strange field. There are red-herring moral arguments about “Terminator/Robot uprising” situations, but we’re still pretty far away from the general AI needed for a Terminator situation.
“The robots are going to take our jobs” is another common refrain. It might be true, but there’s a chance that it’s over-hyped and the amount of people displaced by robotic workers will be much lower than expected, at least in the near (10-20 year) future.<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup>
I’m not trying to determine if it’s true however, so let’s assume for now that it’s not.
There are also more reasonable arguments, such as “Search and Rescue” projects just being “Search and Destroy,” a whole lot of surveillance issues, and, the biggest one to me, who do robots actually help?</p>

<p>At this point it might be useful to take a step back, and note that this is all my subjective experience and opinion, and really only focused on my experience, not anyone else’s.</p>

<p>What the world needs is a pretty philosophical, and usually political, question. It depends on what you mean by “the world,” on what you mean by “need,” and on the context to those two answers.<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup>
In my opinion, “the world” shouldn’t mean the entire world, as untargeted reforms don’t seem to help many people,
and not understanding enough about the context and the community that you’re trying to help means you won’t really help them at all.<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup></p>

<p>There’s enough info about <a href="https://web.stanford.edu/~chadj/IdeaPF.pdf">productivity from scientific research slowing down</a>, likely because we just have really hard problems now.
I don’t think I’m going to become the next Ada Lovelace or Alexander Fleming and invent something as world changing as the generalized programming or penicillin.
So, it’s a much better bet to find a community, listen to their problems and try to understand those problems from their perspective.
I also hold that most people will improve things for themselves if given the chance and ability.
The <a href="https://www.codeforamerica.org/news/filing-taxes-for-economic-justice/">Earned Income Tax Credit</a> is a good example of that.
In short, I think that helping people help themselves is a good philosophy.</p>

<p>For most robotic projects in industry, the answer to “who do these robots actually help” is <a href="https://harvardmagazine.com/2016/05/who-owns-the-robots-rules-the-world">the rich</a>.
I tried to subvert that in a few ways. Over one summer, I designed a curriculum for a robotics class at a day camp for late elementary to early middle schoolers.
While the program was reasonably successful (the instructor we hired to teach the class stayed on with the org running the camp to do more robotics classes), it didn’t satisfy me as much as I had hoped, partially because I wasn’t able to actually teach it. I imagine I’d feel more fulfilled if I had the opportunity to see the curriculum come to life in the classroom.</p>

<p>I have also tried diving heavily into open source software. I was a maintainer with MoveIt for a while, and mentored a Google Summer of Code student for the project.
I had a general vision of software for most robots being free, or close to free. But even in a perfect world, where the software we wrote was easy to use and understand, was bug free, and had all of the features general people wanted, not just robotic specialists, who would actually have access to useful robots?
Even most robotic arms are tens of thousands of dollars, and while there’s some progress being made with <a href="https://www.berkeleyopenarms.org/">Berkeley’s Open Arms</a>, I don’t think there are good incentives for useful robots to be made at consumer level costs.
This makes it difficult to justify to myself that robots can be used and controlled by the masses, and not just by the subset of people with the funds to own robots as capital.
That might be fine with some people, but like I outlined above, it doesn’t align with what I would consider what the world needs.</p>

<p>Like I mentioned earlier, I wrote this post only for myself; what sort of positions my skills and situation would lead me too.
To be clear, I think robotics can be a morally good field, and I would highly encourage anyone reading this to consider a career in robotics if you are interested.
There are applications in space travel like at <a href="https://www.nasa.gov/topics/technology/robotics/index.html">NASA</a> (or other orgs doing space travel that’s not rooted in a desire to escape earth), disaster technology, like the robots deployed to help with <a href="https://www.wired.com/story/fukushima-robot-cleanup/">Fukushima cleanup</a>, and wearables and exoskeletons (check out the <a href="https://biorob2020nyc.org/program/">BiRob2020 proceedings</a> if you’re interested in stuff like this).
There are other robotic applications that focus on sustainability and food generation, but the line between allowing businesses to exploit workers and helping the world gets blurry, and, more importantly, there are plenty of other <a href="https://ironox.com/">capable people</a> in robotics working on those goals, so it’s not necessarily neglected.
If you are interested in any of those above fields, I’d also encourage you to be aware of the effects, both positive and negative, that your work actually has.
If you’re in academia, see who’s citing you, and who’s using your open-source projects.
If you’re a practitioner, think about the effects your company and industry has on the rest of the world, and the communities around you.
Know how you’re perceived, and don’t brush it off as “they don’t understand.”
Know that to fix societal issues, you <em>need</em> a <a href="https://medium.com/civic-tech-thoughts-from-joshdata/so-you-want-to-reform-democracy-7f3b1ef10597">deep understanding of those issues</a>.</p>

<h2 id="whats-next">What’s Next</h2>

<p>Where does that leave us? Staying in robotics really doesn’t seem to mesh philosophically with what I think is good for society, despite how much I like it.
It’s a hard situation, and I have a lot of trouble trying to explain to friends and colleagues why exactly I wanted to change careers.
It’s the main reason that I wrote this post.
Robotics is hot right now, and will be for a long time.
But “what the world needs” still dogged at me.</p>

<p>By February of 2020, I had made up my mind about moving out of robotics into something else, eventually. I had some thoughts about “getting better at solving human problems” jotted down in my work notebook. But the key word was eventually. I had no idea what steps to actually take, or even what problem I wanted to try to solve outside of robotics. But I knew that my next step was to find a small problem that I could engage with and hopefully, make an improvement in. That… is a topic for another post. ;)</p>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>I heard about Ikigai from <a href="https://www.npr.org/2020/10/04/920080747/6-tips-for-making-a-career-change-from-someone-who-has-done-it">an episode</a> of NPR’s Life Kit. I really don’t know any of the cultural background of the term, or whether it’s actually a common way of thinking in Japan, but I like it, and feel like it applies to me. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p>I may have been socially pressured to choose engineering; I group up in a very STEM-centric world, with a general “Engineering is the way” mentality. But I do like to think that I exercised a small amount of free will by choosing programming specifically. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p>This gets complicated with the general exponential rate of change in technology. Before the pandemic, I would have said that most robots aren’t sophisticated enough to take over most people’s jobs, but now with millions unemployed because of the pandemic, automation has been deployed at an <a href="https://www.wired.com/story/covid-brings-automation-workplace-killing-some-jobs/">increasing rate</a>. I suspect that it won’t take long before I’m proven wrong here. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p>While I don’t agree with them all the time, <a href="https://80000hours.org/">80k hours</a> has a lot of good writing about <a href="https://80000hours.org/career-planning/">philosophy in your career choice</a>, and even if you don’t agree with the results of their thinking, just the idea of approaching your career as something to do good with is a great idea. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p>A great example of this is the failed collaboration between Boston Public Schools and MIT to <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3333212">redesign the bus schedule</a>. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="civic" /><category term="career" /><summary type="html"><![CDATA[There’s an Japanese concept about one’s purpose in life called Ikigai.1 It’s built on four criteria: I heard about Ikigai from an episode of NPR’s Life Kit. I really don’t know any of the cultural background of the term, or whether it’s actually a common way of thinking in Japan, but I like it, and feel like it applies to me. &#8617;]]></summary></entry><entry><title type="html">Choosing New Colors for my Website</title><link href="https://brycestevenwilley.github.io/2020/09/choosing_colors" rel="alternate" type="text/html" title="Choosing New Colors for my Website" /><published>2020-09-30T21:50:49-04:00</published><updated>2020-09-30T21:50:49-04:00</updated><id>https://brycestevenwilley.github.io/2020/09/choosing_colors</id><content type="html" xml:base="https://brycestevenwilley.github.io/2020/09/choosing_colors"><![CDATA[<p>I never quite understood how to use colors. I can appreciate them when used well; Blade Runner 2049 was
my favorite movie for a while because of its striking palettes. La La Land still is my favorite for the same reason. But if you asked me
to come up with my own palettes, I’d either be baffled, or would choose the loudest combinations you’ve seen.</p>

<p>When I first started my website in late undergrad, I stuck with simple colors. White background, black text, supporting a dark blue <span style="color:#1565C0;">⬤</span>,
heavily inspired by Google’s <a href="https://material.io/design/">Material Design</a>.</p>

<p><img src="/assets/blogs/colors/original_website.png" alt="My original website design" /></p>

<p>After a few years in grad school, I realized that I needed a ‘brand’: some consistent visual aspect to my online  and professional presence.
I first started deciding on colors to use in my ‘brand’ in some <a href="https://brycewilley.xyz/comp600talk/">presentations</a> , and then redesigned the website around them.</p>

<p>The colors were:</p>

<ul>
  <li>a bright blue <span style="color:#0090EA;">⬤</span> used as the primary background and theme:</li>
  <li>a crazy bright yellow <span style="color:#ffd90a;">⬤</span> to counter the blue</li>
  <li>a dark purple <span style="color:#66298c;">⬤</span> whenever an accent was needed</li>
  <li>white <span style="color:#ffffff;">⬤</span> for text</li>
</ul>

<p><img src="/assets/blogs/colors/website_v2.png" alt="My website before the color redesign" /></p>

<p>I honestly just went with these because:</p>

<ol>
  <li>They stood out amid a web of black text on white backgrounds. I just wanted to be a bit more unique, the true snowflake I am</li>
  <li>They’re fun! Bright colors are just fun for me! I’m by no means a designer by trade, just by hobby. Making my website appeal to anyone but myself wasn’t necessary.</li>
</ol>

<p>However, eventually the colors started to wear on me.</p>

<p>I posted <a href="/visioning_texts/">Visioning Texts</a> on Reddit early this year, and got quite a bit more attention than I was expecting. While there were a lot of garbage comments, the most constructive ones were related to the color choice:</p>

<ul>
  <li>“The blue background is extremely hideous”</li>
  <li>“It’s just too much of a harsh contrast. Makes it difficult to focus on the data”</li>
  <li>“Colour palette could do better. Blue and yellow remind me of IKEA”</li>
</ul>

<p>At first I brushed it off for the same reasons: it’s my project, I can do what I want! And that’s a perfectly valid response! I could have just left it.
But in the back of my head, I knew that I choose these colors knowing completely nothing about color theory, or even just the basics outside of RGB. I had no idea what HSV was, and my main color choosing method was the Material design website and fiddling with hex values.</p>

<p>After coming across <a href="https://www.youtube.com/watch?v=FTKP0Y9MVus">several</a>
  <a href="https://blog.datawrapper.de/beautifulcolors">great</a> <a href="https://blog.datawrapper.de/colorguide">articles</a> and <a href="https://www.youtube.com/watch?v=cPeqyGig0vQ">videos</a>,
I got the motivation push to actually put some effort into my colors this time.</p>

<h2 id="the-actual-work">The Actual Work</h2>

<p>I wanted to keep the same amount of fun with my new color choice, while being a little easier to read and just a simge more serious.
I found the idea of <a href="https://blog.datawrapper.de/colorguide/#4">copying palettes from Movies</a> particularly interesting, so I skipped through some movies/TV shows on Netflix to scenes I remembered being striking, and grabbed a few screenshots <sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>.</p>

<p>I threw all of the screen shots into a collage, and was surprised with the results. I thought I had grabbed more scenes with blues in them, but the only one present was a scene from Neon Genesis: Evangelion.</p>

<p><img src="/assets/blogs/colors/film_collage.jpg" alt="A collage of screenshots from my favorite films" /></p>

<p>On a whim, I generated a palette from it (on <a href="https://www.canva.com/colors/color-palette-generator/">Canva</a>), and got:</p>

<ul>
  <li>Woody brown: <span style="color:#442a2d;">⬤</span></li>
  <li>Whiskey: <span style="color:#d0935d;">⬤</span></li>
  <li>De York: <span style="color:#88c484;">⬤</span></li>
  <li>Deluge: <span style="color:#7a68ac;">⬤</span></li>
</ul>

<p>I played around for a long time in <a href="http://colorizer.org/">colorizer</a> and <a href="https://coolors.co">coolors</a> adjusting brightness values and occasionally the cyan/yellow scales to make sure the colors would still be distinct for colorblind people and got … something.</p>

<p><img src="/assets/blogs/colors/redesign_v3_0.png" alt="The first redesign" /></p>

<p>Which wasn’t the worst, but still seemed disjoint. It clicked after someone pointed out to me that the individual images didn’t have that much in common. I was hoping that I’d just get colors that I liked by putting the images into a collage, but that wasn’t the main point of choosing new colors.</p>

<p>I then just made color pallets of each individual image in the collage and came across a second issue of taking colors from stills of films: the <a href="https://en.wikipedia.org/wiki/Checker_shadow_illusion">checkered shadow illusion</a>: the actual color values in the image aren’t colors that your brain is interpreting.
For example, the bright yellow rain coat in Dark is a staple in the series. In my mind, it always seemed so bright, the only bold color in a sea of washed out grays.
But the direct sample from the image was <span style="color:#a5884f;">⬤</span>. Ew.</p>

<p>Not a ton of the screen shots worked when trying to extract a usable pallet, so I had to mix and match a bit. I combined some of the colors from Blade Runner: 2049 and Evangelion to get the final colors you see here:</p>

<ul>
  <li>A blue grey <span style="color:#d4e0e3;">⬤</span> background</li>
  <li>A green <span style="color:#367026;">⬤</span> and variant <span style="color:#000000;">⬤</span> for links (before and after visiting)</li>
  <li>A dark purple <span style="color:#3e154f;">⬤</span> for the nav bar and body text</li>
  <li>A lighter fuchsia <span style="color:#e4b8da;">⬤</span> for nav bar text</li>
</ul>

<p>I also made the header images grayscale to make them smaller (faster load times) and to blend better with the grey background.</p>

<p><img src="/assets/blogs/colors/redesign_v3_1.png" alt="The final redesign" /></p>

<p>My main takeaways?:</p>

<ul>
  <li>Don’t be afraid to copy color palettes.</li>
  <li>Don’t take too much inspiration from too many places.</li>
  <li>As with everything, keep iterating til your happy, never just try one or two.</li>
</ul>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p>The list was Dark, Neon Genesis: Evangelion, Black Mirror (USS Calister), Blade Runner 2049, and Eternal Sunshine of the Spotless Mind. Took about 40 minutes to skip through the episodes/movies and grab the screen shots. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name></name></author><category term="design" /><category term="colors" /><category term="films" /><summary type="html"><![CDATA[I never quite understood how to use colors. I can appreciate them when used well; Blade Runner 2049 was my favorite movie for a while because of its striking palettes. La La Land still is my favorite for the same reason. But if you asked me to come up with my own palettes, I’d either be baffled, or would choose the loudest combinations you’ve seen.]]></summary></entry></feed>